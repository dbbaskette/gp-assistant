spring:
  application:
    name: gp-assistant
  datasource:
    url: ${DB_URL:jdbc:postgresql://localhost:15432/gp_assistant}
    username: ${DB_USERNAME:gpadmin}
    password: ${DB_PASSWORD:VMware1!}
    hikari:
      maximum-pool-size: 20
      minimum-idle: 5
      connection-timeout: 30000
      idle-timeout: 600000
  ai:
    retry:
      max-attempts: 1
      backoff:
        initial-interval: 1000
    # OpenAI configuration - defaults are set by Application.java based on presence of OPENAI_API_KEY
    # Local models (default): http://127.0.0.1:1234
    # OpenAI models: requires OPENAI_API_KEY environment variable
    openai:
      embedding:
        options:
          dimensions: 768
    mcp:
      client:
        # MCP Client configuration
        # Set SPRING_AI_MCP_CLIENT_ENABLED=true in environment to enable MCP tools
        # App will start successfully even if servers are offline
        # Automatic retry with exponential backoff will attempt reconnection
        enabled: ${SPRING_AI_MCP_CLIENT_ENABLED:false}
        type: SYNC
        request-timeout: 30s
        annotation-scanner:
          enabled: true
        toolcallback:
          # Enable MCP tool integration
          enabled: true
        # Streamable HTTP transport - supports multiple server connections
        # Note: Connections are managed dynamically by DynamicMcpClientManager
        # to allow graceful startup when servers are offline
        streamable-http:
          connections: {}

# Flyway configuration
flyway:
  enabled: true

# Application-specific configuration
app:
  docs:
    # URL of the Greenplum documentation PDF to ingest
    pdf-url: https://techdocs.broadcom.com/content/dam/broadcom/techdocs/us/en/pdf/vmware-tanzu/data-solutions/tanzu-greenplum/7/greenplum-database/greenplum-database.pdf
    # Enable automatic ingestion on startup (default: true)
    ingest-on-startup: ${DOCS_INGEST_ON_STARTUP:true}
    # Batch size for embedding generation (reduce if you get timeouts)
    batch-size: ${DOCS_BATCH_SIZE:50}
    # Pages to process per batch during PDF parsing
    pages-per-batch: ${DOCS_PAGES_PER_BATCH:50}
  vectorstore:
    # Force embedding dimension used for schema creation (match your embedding model)
    dimensions: ${APP_VECTORSTORE_DIMENSIONS:1536}
  rag:
    # Maximum number of relevant documents to retrieve
    top-k: ${RAG_TOP_K:5}
    # Similarity threshold (0.0 to 1.0)
    similarity-threshold: ${RAG_SIMILARITY_THRESHOLD:0.7}

# Actuator configuration
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
  endpoint:
    health:
      show-details: always
  metrics:
    tags:
      application: ${spring.application.name}
    export:
      prometheus:
        enabled: true

# Server configuration
server:
  # Tomcat timeout configurations (aligned with IMC-chatbot)
  tomcat:
    connection-timeout: 30000
    keep-alive-timeout: 30000
  # HTTP client configuration for Spring AI calls
  mvc:
    async:
      request-timeout: 60000  # 60 seconds

# RestClient configuration
spring.http.client.connect-timeout: 10s
spring.http.client.read-timeout: 60s

# Logging configuration
logging:
  level:
    root: INFO
    com.baskettecase.gpassistant: DEBUG
    org.springframework.ai: TRACE
    org.springframework.ai.vectorstore: TRACE
    org.springframework.ai.openai: TRACE
    org.springframework.web.reactive.function.client: DEBUG
    reactor.netty.http.client: DEBUG
    org.springframework.web.client: DEBUG
    org.flywaydb: INFO
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} - %logger{36} - %msg%n"
  file:
    name: ${APP_LOG_FILE:}
